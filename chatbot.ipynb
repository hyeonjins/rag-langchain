{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chatbot\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# llm\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "\n",
        "chat_template = \"\"\"\n",
        "나는 개인정보 유출 관련 상담을 진행하는 챗봇이다\n",
        "\n",
        "0. 사용자의 상담내용에 집중하고 그에 맞는 답변을 자세히 알려줘야한다.\n",
        "1. 사례를 들어서 설명한다.\n",
        "2. 친절하게 답변한다.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "chat_ai = ChatOpenAI(temperature=0.5, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "# Retrieval QA Chain 생성\n",
        "loader = PyPDFLoader(\"data/2021.pdf\")\n",
        "# Split the text in chunks, using LangChain Recursive Character Text Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        "    )\n",
        "\n",
        "pages = loader.load_and_split(text_splitter)\n",
        "\n",
        "directory = 'data'\n",
        "vector_index = Chroma.from_documents(\n",
        "    pages, # Documents\n",
        "    OpenAIEmbeddings(), # Text embedding model\n",
        "    persist_directory=directory # persists the vectors to the file system\n",
        "    )\n",
        "\n",
        "vector_index.persist()\n",
        "\n",
        "retriever = vector_index.as_retriever(\n",
        "    search_type=\"similarity\", # Cosine Similarity\n",
        "    search_kwargs={\n",
        "        \"k\": 3, # Select top k search results\n",
        "    }\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True # source document which were used as source files\n",
        ")\n",
        "\n",
        "\n",
        "# 이전 대화 내용을 저장할 변수\n",
        "previous_conversations = []\n",
        "\n",
        "def qa_response(message, history):\n",
        "    global previous_conversations  # 전역 변수를 함수 내에서 사용하기 위해 선언\n",
        "\n",
        "    # 대화가 계속될 경우 이전 대화 내용에 추가하고 응답 생성\n",
        "    chats = qa_chain.invoke(message)\n",
        "    previous_conversations.append((\"사용자\", message))\n",
        "    previous_conversations.append((\"AI\", chats[\"result\"]))\n",
        "\n",
        "    # 대화종료를 입력하면 최종 요약 출력\n",
        "    if message == \"대화종료\":\n",
        "        # 최종 요약 생성\n",
        "        summary = summarize(previous_conversations)\n",
        "        return summary\n",
        "\n",
        "    return chats[\"result\"]\n",
        "\n",
        "# 요약 llm 모델 추가\n",
        "# summarize_llm과 history로 출력\n",
        "def summarize(history):\n",
        "    # 사용자의 대화와 AI 대답을 모두 정리해서 요약\n",
        "    conversation_text = \"\\n\".join([f\"{speaker} : {utterance}\" for speaker, utterance in history])\n",
        "\n",
        "    summarize_llm = OpenAI(\n",
        "        temperature=0.3, model=\"gpt-3.5-turbo-instruct\", max_tokens=512\n",
        "    )\n",
        "\n",
        "    summarize_template = \"\"\"\n",
        "    필수 : 사용자의 대화와 AI 대답을 모두 정리해서 요약해줘.\n",
        "    \"\"\"\n",
        "\n",
        "    summarize_prompt = PromptTemplate(\n",
        "        template=summarize_template, input_variables=[\"texts\"]\n",
        "    )\n",
        "    summarize_chain = LLMChain(prompt=summarize_prompt, llm=summarize_llm)\n",
        "\n",
        "    return summarize_chain.run(conversation_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gr.ChatInterface(\n",
        "    fn=qa_response, \n",
        "    textbox=gr.Textbox(placeholder=\"채팅을 입력해주세요..\", container=False, scale=7),\n",
        "    # 채팅창의 크기를 조절한다.\n",
        "    chatbot=gr.Chatbot(height=1000),\n",
        "    title=\"개인정보 유출 피해 상담 챗봇\",\n",
        "    description='대화가 끝나면 \"대화종료\" 를 입력해주세요',\n",
        "    theme=\"soft\",\n",
        "    examples=[[\"공공기관에서 동의 없이 개인정보를 경찰서 같은 수사기관에 제공했는데 이 행위에 대해서 손해배상을 요구할 수 있나요?\"], [\"저작권 관련해서\"]],\n",
        "    retry_btn=\"다시보내기 ↩\",\n",
        "    undo_btn=\"이전챗 삭제 ❌\",\n",
        "    clear_btn=\"전챗 삭제 💫\",\n",
        ").launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
