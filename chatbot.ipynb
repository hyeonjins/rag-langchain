{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# chatbot\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "# llm\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "import gradio as gr\n",
        "import os\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "\n",
        "chat_template = \"\"\"\n",
        "ë‚˜ëŠ” ê°œì¸ì •ë³´ ìœ ì¶œ ê´€ë ¨ ìƒë‹´ì„ ì§„í–‰í•˜ëŠ” ì±—ë´‡ì´ë‹¤\n",
        "\n",
        "0. ì‚¬ìš©ìì˜ ìƒë‹´ë‚´ìš©ì— ì§‘ì¤‘í•˜ê³  ê·¸ì— ë§ëŠ” ë‹µë³€ì„ ìì„¸íˆ ì•Œë ¤ì¤˜ì•¼í•œë‹¤.\n",
        "1. ì‚¬ë¡€ë¥¼ ë“¤ì–´ì„œ ì„¤ëª…í•œë‹¤.\n",
        "2. ì¹œì ˆí•˜ê²Œ ë‹µë³€í•œë‹¤.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "chat_ai = ChatOpenAI(temperature=0.5, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "# Retrieval QA Chain ìƒì„±\n",
        "loader = PyPDFLoader(\"data/2021.pdf\")\n",
        "# Split the text in chunks, using LangChain Recursive Character Text Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        "    )\n",
        "\n",
        "pages = loader.load_and_split(text_splitter)\n",
        "\n",
        "directory = 'data'\n",
        "vector_index = Chroma.from_documents(\n",
        "    pages, # Documents\n",
        "    OpenAIEmbeddings(), # Text embedding model\n",
        "    persist_directory=directory # persists the vectors to the file system\n",
        "    )\n",
        "\n",
        "vector_index.persist()\n",
        "\n",
        "retriever = vector_index.as_retriever(\n",
        "    search_type=\"similarity\", # Cosine Similarity\n",
        "    search_kwargs={\n",
        "        \"k\": 3, # Select top k search results\n",
        "    }\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True # source document which were used as source files\n",
        ")\n",
        "\n",
        "\n",
        "# ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•  ë³€ìˆ˜\n",
        "previous_conversations = []\n",
        "\n",
        "def qa_response(message, history):\n",
        "    global previous_conversations  # ì „ì—­ ë³€ìˆ˜ë¥¼ í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„ ì–¸\n",
        "\n",
        "    # ëŒ€í™”ê°€ ê³„ì†ë  ê²½ìš° ì´ì „ ëŒ€í™” ë‚´ìš©ì— ì¶”ê°€í•˜ê³  ì‘ë‹µ ìƒì„±\n",
        "    chats = qa_chain.invoke(message)\n",
        "    previous_conversations.append((\"ì‚¬ìš©ì\", message))\n",
        "    previous_conversations.append((\"AI\", chats[\"result\"]))\n",
        "\n",
        "    # ëŒ€í™”ì¢…ë£Œë¥¼ ì…ë ¥í•˜ë©´ ìµœì¢… ìš”ì•½ ì¶œë ¥\n",
        "    if message == \"ëŒ€í™”ì¢…ë£Œ\":\n",
        "        # ìµœì¢… ìš”ì•½ ìƒì„±\n",
        "        summary = summarize(previous_conversations)\n",
        "        return summary\n",
        "\n",
        "    return chats[\"result\"]\n",
        "\n",
        "# ìš”ì•½ llm ëª¨ë¸ ì¶”ê°€\n",
        "# summarize_llmê³¼ historyë¡œ ì¶œë ¥\n",
        "def summarize(history):\n",
        "    # ì‚¬ìš©ìì˜ ëŒ€í™”ì™€ AI ëŒ€ë‹µì„ ëª¨ë‘ ì •ë¦¬í•´ì„œ ìš”ì•½\n",
        "    conversation_text = \"\\n\".join([f\"{speaker} : {utterance}\" for speaker, utterance in history])\n",
        "\n",
        "    summarize_llm = OpenAI(\n",
        "        temperature=0.3, model=\"gpt-3.5-turbo-instruct\", max_tokens=512\n",
        "    )\n",
        "\n",
        "    summarize_template = \"\"\"\n",
        "    í•„ìˆ˜ : ì‚¬ìš©ìì˜ ëŒ€í™”ì™€ AI ëŒ€ë‹µì„ ëª¨ë‘ ì •ë¦¬í•´ì„œ ìš”ì•½í•´ì¤˜.\n",
        "    \"\"\"\n",
        "\n",
        "    summarize_prompt = PromptTemplate(\n",
        "        template=summarize_template, input_variables=[\"texts\"]\n",
        "    )\n",
        "    summarize_chain = LLMChain(prompt=summarize_prompt, llm=summarize_llm)\n",
        "\n",
        "    return summarize_chain.run(conversation_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gr.ChatInterface(\n",
        "    fn=qa_response, \n",
        "    textbox=gr.Textbox(placeholder=\"ì±„íŒ…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”..\", container=False, scale=7),\n",
        "    # ì±„íŒ…ì°½ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•œë‹¤.\n",
        "    chatbot=gr.Chatbot(height=1000),\n",
        "    title=\"ê°œì¸ì •ë³´ ìœ ì¶œ í”¼í•´ ìƒë‹´ ì±—ë´‡\",\n",
        "    description='ëŒ€í™”ê°€ ëë‚˜ë©´ \"ëŒ€í™”ì¢…ë£Œ\" ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”',\n",
        "    theme=\"soft\",\n",
        "    examples=[[\"ê³µê³µê¸°ê´€ì—ì„œ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ê²½ì°°ì„œ ê°™ì€ ìˆ˜ì‚¬ê¸°ê´€ì— ì œê³µí–ˆëŠ”ë° ì´ í–‰ìœ„ì— ëŒ€í•´ì„œ ì†í•´ë°°ìƒì„ ìš”êµ¬í•  ìˆ˜ ìˆë‚˜ìš”?\"], [\"ì €ì‘ê¶Œ ê´€ë ¨í•´ì„œ\"]],\n",
        "    retry_btn=\"ë‹¤ì‹œë³´ë‚´ê¸° â†©\",\n",
        "    undo_btn=\"ì´ì „ì±— ì‚­ì œ âŒ\",\n",
        "    clear_btn=\"ì „ì±— ì‚­ì œ ğŸ’«\",\n",
        ").launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
